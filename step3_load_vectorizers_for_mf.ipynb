{"nbformat": 4, "nbformat_minor": 0, "metadata": {"colab": {"provenance": [{"file_id": "1uxGz7-Qz7aiJLqWXk_QX4O6mrXK7NM56", "timestamp": 1716770380642}], "machine_shape": "hm"}, "kernelspec": {"name": "python3", "display_name": "Python 3"}, "language_info": {"name": "python"}}, "cells": [{"cell_type": "code", "execution_count": null, "metadata": {"id": "O-pyOMaZWYsk", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "d07e7c19-6de3-4cd3-fe41-09821ca2e1c8"}, "outputs": [], "source": ["# Connecting Google Drive\n", "from google.colab import drive\n", "drive.mount('/content/drive')"]}, {"cell_type": "code", "source": ["# Importing all libraries\n", "import pandas as pd\n", "from tqdm import tqdm\n", "from nltk.corpus import words\n", "import pickle\n", "from gensim import corpora\n", "from sklearn.feature_extraction.text import CountVectorizer"], "metadata": {"id": "hBK1qsa0St8o"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["# Address to project and dataset folder\n", "project_folder = \"/content/drive/MyDrive/2024SUDSProject/\"\n", "dataset_folder = \"/content/drive/MyDrive/2024SUDSProject/datasets/\""], "metadata": {"id": "KTZpTxEu8Q5q"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["def save_tokens(year, all_tokens):\n", "    # Save all_tokens variable into pickle file\n", "    with open(dataset_folder + f'step3_all_tokens_{year}.pkl', 'wb') as file:\n", "        pickle.dump(all_tokens, file)"], "metadata": {"id": "OEcd8C5lI9H2"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["def save_seed_topics(year, seed_topics):\n", "    # Save seed_topics variable into pickle file\n", "    with open(dataset_folder + f'step3_seed_topics_{year}.pkl', 'wb') as f:\n", "        pickle.dump(seed_topics, f)"], "metadata": {"id": "ds9YvQYn_nVO"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["def save_vectorizer(year, vectorizer):\n", "    # Save vectorizer into pickle file\n", "    with open(dataset_folder + f'step3_vectorizer_{year}.pkl', 'wb') as file:\n", "        pickle.dump(vectorizer, file)"], "metadata": {"id": "3yVvK425z5Mv"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["def save_data_vectorized(year, data_vectorized):\n", "    # Save data_vectorized into pickle file\n", "    with open(dataset_folder + f'step3_data_vectorized_{year}.pkl', 'wb') as file:\n", "        pickle.dump(data_vectorized, file)"], "metadata": {"id": "3Y9xAqIzQ1Ik"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["def save_dictionary(year, dictionary):\n", "    # Save dictionary into pickle file\n", "    with open(dataset_folder + f'step3_dictionary_{year}.pkl', 'wb') as file:\n", "        pickle.dump(dictionary, file)"], "metadata": {"id": "rtRfFOWvnaiH"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["def save_bow(year, bow):\n", "    # Save data_vectorized into pickle file\n", "    with open(dataset_folder + f'step3_bow_{year}.pkl', 'wb') as file:\n", "        pickle.dump(bow, file)"], "metadata": {"id": "7gVF1EMbBMKG"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["# Opens Moral Foundation Dictionary. Contains dictionary of words along with their\n", "# correlated scores and foundation.\n", "emfd = pd.read_csv(dataset_folder + 'emfd_amp.csv')\n", "\n", "\n", "# Creates list of foundations such as care.virtue, care.vice, loyalty.vice, etc.\n", "# Note: There are 5 moral foundation categories, each with a virtue and vice branch\n", "unique_foundations = emfd[\"foundation\"].unique().tolist()\n", "\n", "# Compiles dictionary to find the top 10 words associated with\n", "# each moral foundation category, stores the within moral_foundation_seed_dict\n", "\n", "moral_foundation_seed_dict = {}\n", "\n", "for foundation in unique_foundations:\n", "    columns = [col for col in emfd.columns if foundation in col]\n", "    top_words = []\n", "\n", "    # Consider including more words to improve accuracy?\n", "    for col in columns:\n", "        top_words += emfd.nlargest(10, col)['word'].tolist()\n", "\n", "    moral_foundation_seed_dict[foundation] = top_words\n"], "metadata": {"id": "rLHBDaeM723v"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["def create_id2word(all_tokens):\n", "    # Creates dictionary that maps words to integer ids\n", "    id2word = corpora.Dictionary(all_tokens)\n", "\n", "    # This line is unused but may be useful later\n", "    # corpus = [id2word.doc2bow(text) for text in tqdm(all_tokens, desc=\"Creating Corpus\")]\n", "\n", "    return id2word\n", "\n", "def create_bow(dictionary, all_tokens):\n", "    # Creates bag of words\n", "    bow = [dictionary.doc2bow(text) for text in tqdm(all_tokens, desc=\"Creating Bag of Words\")]\n", "\n", "    return bow"], "metadata": {"id": "oRQT3bPS8Msb"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["def create_vectorizers(docs_strings):\n", "\n", "    # Creates vectorizer as tool to transform data into a matrix of token frequency per sentence\n", "    vectorizer = CountVectorizer(analyzer='word', lowercase=False)\n", "    data_vectorized = vectorizer.fit_transform(tqdm(docs_strings, desc=\"Vectorizing\"))\n", "\n", "    return vectorizer, data_vectorized\n", "\n", "def create_seed_topics(id2word):\n", "    seed_topics = {}\n", "    for topic_id, seed_words in tqdm(enumerate(moral_foundation_seed_dict.values()), desc=\"Seed Topics\"):\n", "        for word in seed_words:\n", "            # Creates a word_id for each word in the seed_words list, checks if its exists\n", "            # If it doesn't it returns None\n", "            word_id = id2word.token2id.get(word, None)\n", "            if word_id is not None:\n", "                # Assigns the word_id to its corresponding moral foundation category num\n", "                seed_topics[word_id] = topic_id\n", "\n", "    return seed_topics\n", "\n", "def open_dict(year):\n", "    with open(dataset_folder + f'step3_dictionary_{year}.pkl', 'rb') as file:\n", "        dictionary = pickle.load(file)\n", "    return dictionary\n"], "metadata": {"id": "FBq_kOAqL10J"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["# years = ['2017', '2018', '2019', '2020', '2021', '2022']\n", "\n", "years = ['2019', '2020', '2021', '2022']\n", "\n", "def doc_generator(docs):\n", "  for doc in docs:\n", "    yield str(doc).split()\n", "\n", "years = ['sample']\n", "\n", "# Iterate over chunks of the CSV file\n", "# Stores all tokens in all_tokens\n", "for year in years:\n", "    # Initialize an empty list to store processed tokens\n", "    doc_strings = []\n", "\n", "    df = pd.read_csv(dataset_folder + f'combined_data_preprocessed_{year}_lemma.csv')\n", "\n", "    for index, row in df.iterrows():\n", "        doc_strings.append(str(row['content']))\n", "\n", "    id2word = create_id2word(doc_generator(df['content']))\n", "\n", "    bow = create_bow(id2word, doc_generator(df['content']))\n", "\n", "    save_dictionary(year, id2word)\n", "    save_bow(year, bow)\n", "\n", "    seed_topics = create_seed_topics(id2word)\n", "\n", "    vectorizer, data_vectorized = create_vectorizers(doc_strings)\n", "\n", "    save_seed_topics(year, seed_topics)\n", "    save_data_vectorized(year, data_vectorized)\n", "    save_vectorizer(year, vectorizer)\n", "    # save_tokens(year, all_tokens) removed, b/c not memory efficient\n"], "metadata": {"id": "HsCqFdjnNCS4", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "1196de2c-00ad-4ed0-e196-6b0f87d26716"}, "execution_count": null, "outputs": []}]}