{"cells": [{"cell_type": "markdown", "metadata": {"id": "m22ruLTX6tOg"}, "source": ["**Instructions**\n", "\n", "To run this file, run each cell sequentially from top to bottom. There are cells at the bottom of the notebook which are no longer used, so don't run them. I'm just keeping them there for record.\n", "\n", "**Runtime**\n", "\n", "Cleaning one year has an approximate runtime of ~9 hrs, but it may be faster.\n", "\n", "**Tips**\n", "\n", "Google Colab is prone to disconnecting if your computer falls asleep or your wifi disconnects, so make sure your computer is on and your internet connection is stable."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "N2u1hJxKYUAU", "outputId": "5f34379f-8bd3-4350-960f-985e0922cd14"}, "outputs": [], "source": ["from google.colab import drive\n", "drive.mount('/content/drive')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import spacy\n", "import string\n", "import pandas as pd\n", "import nltk\n", "from tqdm import tqdm\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Configure paths and runtime settings\n", "dataset_folder = \"/content/drive/MyDrive/2024SUDSProject/datasets/\"\n", "years = ['2022']\n", "chunk_size = 2000  # Tune based on memory\n", "use_english_words = True\n", "spacy_batch_size = 50\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Creates table of characters removed from text such as numbers and punctuation\n", "char_removal_dict = {}\n", "for char in string.printable:\n", "    if char not in string.ascii_letters and char not in string.whitespace:\n", "        char_removal_dict[char] = ''\n", "char_removal_dict['\\n'] = ''\n", "removal_table = str.maketrans(char_removal_dict)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load spaCy with only the components needed for lemmatization\n", "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Create set of english words for word cleaning (optional)\n", "english_words_set = None\n", "if use_english_words:\n", "    nltk.download('words', quiet=True)\n", "    from nltk.corpus import words\n", "    english_words_set = set(words.words())\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def normalize_text(text):\n", "    text = str(text).lower()\n", "    return text.translate(removal_table)\n", "\n", "def lemmatize_texts(texts):\n", "    docs = nlp.pipe(texts, batch_size=spacy_batch_size)\n", "    lemmatized = []\n", "    for doc in docs:\n", "        if english_words_set is None:\n", "            tokens = [token.lemma_ for token in doc if not token.is_stop]\n", "        else:\n", "            tokens = [token.lemma_ for token in doc if not token.is_stop and token.text in english_words_set]\n", "        lemmatized.append(tokens)\n", "    return lemmatized\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Stream chunks to disk to keep memory usage bounded\n", "for year in years:\n", "    input_path = dataset_folder + f'combined_data_{year}.csv'\n", "    output_path = dataset_folder + f'combined_data_preprocessed_{year}_lemma.csv'\n", "    first_write = True\n", "\n", "    for chunk in tqdm(pd.read_csv(input_path, chunksize=chunk_size), miniters=1, desc='Loading data'):\n", "        texts = chunk['content'].fillna('').map(normalize_text).tolist()\n", "        chunk['content'] = lemmatize_texts(texts)\n", "\n", "        chunk.to_csv(\n", "            output_path,\n", "            index=False,\n", "            mode='w' if first_write else 'a',\n", "            header=first_write,\n", "        )\n", "        first_write = False\n"]}], "metadata": {"colab": {"provenance": [{"file_id": "1a5x4EMBWMWIkbYHGPyViF97Dcqzgyr-C", "timestamp": 1716768345708}]}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.7"}}, "nbformat": 4, "nbformat_minor": 0}