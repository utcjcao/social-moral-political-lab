{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "kQitL_qyVeCp", "outputId": "4d34c504-685e-48b6-c680-5e1e39475f3e"}, "outputs": [], "source": ["# Mount Google Drive\n", "from google.colab import drive\n", "drive.mount('/content/drive')"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "AbFchJjfLkta", "outputId": "98c08d8d-0b63-4ff4-8957-ca1a6af48dbe"}, "outputs": [], "source": ["!pip install sentence_transformers\n", "\n", "import pandas as pd\n", "import numpy as np\n", "from collections import Counter\n", "from sentence_transformers import SentenceTransformer, util\n", "from transformers import BertModel, BertTokenizer\n", "import torch\n", "import matplotlib.pyplot as plt\n", "from scipy.stats import t, sem"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "4pFyIpztTPP5", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "9f2b2412-1c86-4a16-be29-cf7e8034231a"}, "outputs": [], "source": ["# Address to project folder\n", "project_folder = \"/content/drive/MyDrive/2024SUDSProject/Threat/\"\n", "dataset_folder = \"/content/drive/MyDrive/2024SUDSProject/processedNewsData/\"\n", "\n", "df = pd.read_csv(dataset_folder+'combined_honor_score.csv')\n", "\n", "# Step 6: Display the column values\n", "print(df.head())  # Print column names"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "pBbkR3DJLxin"}, "outputs": [], "source": ["# Merge datasets and process labels\n", "def process_labels(tokens_full, orientations):\n", "    # I don't think pd.merge reads in a csv/pickle file\n", "    # What data is contained within orientations?\n", "    tokens_full.rename(columns={'file_name': 'outlet'}, inplace=True)  # Rename file_name column to outlet\n", "    tokens = pd.merge(tokens_full, orientations, left_on='outlet', right_on='source (Master List)', how='left')\n", "    tokens = tokens[['outlet', 'content', 'Media Bias/Fact Check Label', 'id']]\n", "    tokens.rename(columns={'Media Bias/Fact Check Label': 'label'}, inplace=True)\n", "    tokens['label'].fillna('Unknown', inplace=True)\n", "    return tokens\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "zuv9WwQyL0ZG"}, "outputs": [], "source": ["# Extract top words and scores from EMFD\n", "def get_top_words_scores(emfd):\n", "\n", "    scope = [\n", "        'word', 'care.virtue', 'fairness.virtue', 'loyalty.virtue', 'authority.virtue', 'sanctity.virtue',\n", "        'care.vice', 'fairness.vice', 'loyalty.vice', 'authority.vice', 'sanctity.vice', 'foundation'\n", "    ]\n", "\n", "    emfd_scope = emfd[scope]\n", "    top_words_scores_dict = {}\n", "\n", "    for col in emfd_scope.columns[1:-1]:\n", "        words = emfd_scope.loc[emfd_scope[col] != 0, ['word', col]]\n", "        top_words_scores_dict[col] = list(zip(words['word'], words[col]))\n", "\n", "    return top_words_scores_dict\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "LlxvgIhAL3nI"}, "outputs": [], "source": ["# Load threat words from file\n", "def load_threat_words(filepath):\n", "    with open(filepath, 'r') as file:\n", "        threats = [line.strip() for line in file.readlines()]\n", "    return threats\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "kE1TnH9CL8O3"}, "outputs": [], "source": ["# Get threat embeddings using BERT\n", "def get_threat_embeddings(threats):\n", "    model = BertModel.from_pretrained('bert-base-uncased')\n", "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n", "    tokens = tokenizer(threats, return_tensors='pt', padding=True, truncation=True)\n", "    with torch.no_grad():\n", "        outputs = model(**tokens)\n", "        last_hidden_states = outputs.last_hidden_state\n", "    return last_hidden_states.mean(dim=1).numpy()\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "ZgO_accMLtCu"}, "outputs": [], "source": ["# Load datasets\n", "def load_data():\n", "    orientations = pd.read_csv(dataset_folder+'labels.csv')\n", "    emfd = pd.read_csv(dataset_folder+'emfd_amp.csv')\n", "    top_words_scores_dict = get_top_words_scores(emfd)\n", "    threats = load_threat_words(dataset_folder+'threat_corpus.txt')\n", "    # # Is this a csv file or a pickle file?\n", "    # year_dataset = pd.read_csv(project_folder+f'{year}_processed.csv')\n", "    return orientations, emfd, top_words_scores_dict, threats\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "mctjccCuL_I9"}, "outputs": [], "source": ["# Group processed text by media bias label\n", "def group_text_by_label(tokens):\n", "    grouped = tokens.groupby('Media Bias/Fact Check Label')['content'].apply(list)\n", "    return grouped[grouped.index != 'Unknown']\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "J2OFzifkMBcg"}, "outputs": [], "source": ["# Process string into a list of words\n", "def process_string(input_string):\n", "    return str(input_string).split()\n", "    # return [word.strip() for word in input_string.split(' ')]\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "pq7VI-B1MEQN"}, "outputs": [], "source": ["from collections import Counter\n", "\n", "# get the proportion from the article level\n", "# look at freq for moral foundations\n", "\n", "def get_overall_proportion(target_list, df):\n", "    # Convert target_list to a set for faster membership checking\n", "    target_set = set(target_list)\n", "\n", "    # Initialize lists to store results\n", "    threat_proportional_scores = []\n", "    total_word_counts = []\n", "\n", "    for content in df['content'].astype(str):\n", "        # Count the occurrences of each word in the current row's content\n", "        counter = Counter(content.split())\n", "\n", "        # Calculate the total number of words in the current row's content\n", "        total_words = sum(counter.values())\n", "\n", "        # Calculate the number of occurrences of words from target_list in the current row's content\n", "        target_list_occurences = sum(counter[word] for word in target_set if word in counter)\n", "\n", "        # Calculate the proportion for the current row\n", "        if total_words > 0:\n", "            proportion = target_list_occurences / total_words\n", "        else:\n", "            proportion = 0  # Handle division by zero if the sublist is empty\n", "\n", "        # Append results to lists\n", "        threat_proportional_scores.append(proportion)\n", "        total_word_counts.append(total_words)\n", "\n", "    # Assign lists to new columns in the DataFrame\n", "    df['threat_proportional_score'] = threat_proportional_scores\n", "    df['total_word_count'] = total_word_counts\n", "\n", "    return df\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "IGa1yGpkMKeM"}, "outputs": [], "source": ["# Visualize data\n", "def calculate_average_and_ci(data):\n", "    means = np.mean(data, axis=0)\n", "    conf_int = t.interval(0.95, len(data) - 1, loc=means, scale=sem(data, axis=0))\n", "    return means, conf_int\n", "\n", "def visualize_data(result_dict, label_mapping, year):\n", "    for graph_key, smaller_dict in result_dict.items():\n", "        x_labels = list(smaller_dict.keys())\n", "        averages = []\n", "        conf_intervals = []\n", "        for key, values in smaller_dict.items():\n", "            average, ci = calculate_average_and_ci(values)\n", "            averages.append(average)\n", "            conf_intervals.append(np.diff(ci) / 2)\n", "        mapped_x_labels = [label_mapping[int(label)] for label in x_labels]\n", "        fig, ax = plt.subplots()\n", "        ax.bar(mapped_x_labels, averages, yerr=np.array(conf_intervals).T, capsize=5, align='center', alpha=0.7)\n", "        plt.xlabel('Mapped Values')\n", "        plt.ylabel('Average')\n", "        plt.title(f'Bar Plot for Graph: {graph_key}')\n", "        plt.savefig(project_folder + f'figures/threat_{graph_key}_distribution_{year}.png')\n", "        plt.show()\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "MFKfBtKAMHLI"}, "outputs": [], "source": ["def label_to_text_dict_creator(dataframe):\n", "    split_strings = dataframe.astype(str).str.split()\n", "    # used to use iteritems, use items\n", "    return split_strings.to_dict()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "tlUeGi58DxFf"}, "outputs": [], "source": ["# Main function\n", "\n", "years = ['2019', '2020', '2021', '2022']\n", "def main():\n", "\n", "    # Depending on size of dataset, might be inefficient to open dataset in its\n", "    # entirety. Might need to read in chunks? Depends on how long it takes to\n", "    # read in pickle/csv file.\n", "\n", "    # Load in datasets\n", "    orientations, emfd, top_words_scores_dict, threats = load_data()\n", "\n", "    for year in years:\n", "        year_dataset = pd.read_csv(dataset_folder+f'combined_data_preprocessed_{year}_lemma.csv')\n", "\n", "        processed_df = process_labels(year_dataset, orientations)\n", "\n", "        final_df = get_overall_proportion(threats, processed_df)\n", "\n", "        final_df.to_csv(dataset_folder+f'threat_data_score_{year}.csv', index=True, header=True)\n", "\n", "        orientation_averages = {'left':0, 'left_center':0, 'center':0, 'right_center':0, 'right':0,\n", "        'pro-science':0, 'conspiracy_pseudoscience':0, 'questionable_source':0, 'satire':0}\n", "\n", "        for key in orientation_averages:\n", "          total_word_count = final_df[final_df['label'] == key]['total_word_count'].sum()\n", "          if total_word_count != 0:\n", "              orientation_averages[key] = final_df[final_df['label'] == key]['threat_proportional_score'].sum() / total_word_count\n", "          else:\n", "              orientation_averages[key] = 0\n", "\n", "        df = pd.DataFrame(list(orientation_averages.items()), columns=['label', 'average_score'])\n", "\n", "        df.to_csv(dataset_folder+f'orientation_threat_avg_{year}.csv', index=True, header=True)\n", "        # visualize_data(result2022, label_mapping)\n", "\n", "if __name__ == \"__main__\":\n", "    main()\n"]}], "metadata": {"colab": {"machine_shape": "hm", "provenance": []}, "kernelspec": {"display_name": "Python 3", "name": "python3"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 0}