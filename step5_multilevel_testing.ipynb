{"cells": [{"cell_type": "code", "execution_count": null, "id": "a025d2fe", "metadata": {"vscode": {"languageId": "r"}, "id": "a025d2fe", "outputId": "07f4aa98-baba-46b1-d771-1adedd716a2b", "colab": {"base_uri": "https://localhost:8080/"}}, "outputs": [], "source": ["install.packages(\"pbkrtest\")"]}, {"cell_type": "code", "execution_count": null, "id": "cf882a89", "metadata": {"vscode": {"languageId": "r"}, "id": "cf882a89", "outputId": "fb8f1b96-eb3d-40ab-f1d6-eda2f921593e", "colab": {"base_uri": "https://localhost:8080/"}}, "outputs": [], "source": ["library(lme4)\n", "library(tidyverse)\n", "library(pbkrtest)"]}, {"cell_type": "code", "source": ["install.packages(\"googledrive\")\n", "library(googledrive) #In R runtime we dont have the drive auth function. We have to use a package.\n", "drive_auth(use_oob = TRUE) #Mounting Drive"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "Gegfoxa9xQVB", "outputId": "7c71a7f1-d035-4e21-fea1-f1202f3925e5"}, "id": "Gegfoxa9xQVB", "execution_count": null, "outputs": []}, {"cell_type": "code", "execution_count": null, "id": "a6592012", "metadata": {"vscode": {"languageId": "r"}, "id": "a6592012", "colab": {"base_uri": "https://localhost:8080/", "height": 211}, "outputId": "1bbade60-0206-401f-fecd-1e9f6948765c"}, "outputs": [], "source": ["\n", "\n", "merged_df <- do.call(rbind, all_data)\n", "\n", "# If you need to reset row names to avoid duplicate row names\n", "rownames(merged_df) <- NULL\n", "\n", "merged_df$Outlet <- sapply(strsplit(merged_df$Filename, '--'), '[', 1)\n"]}, {"cell_type": "code", "execution_count": null, "id": "8061db1c", "metadata": {"vscode": {"languageId": "r"}, "id": "8061db1c", "outputId": "85f85524-bb25-461c-f6fb-f5a151a164b8", "colab": {"base_uri": "https://localhost:8080/", "height": 106}}, "outputs": [], "source": ["unique(merged_df$Outlet)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"vscode": {"languageId": "r"}, "id": "Sm45ubLYqZi8", "outputId": "0b85ce26-0ea5-4e7d-e196-5f92953aa535"}, "outputs": [], "source": ["unique(merged_df$label)"], "id": "Sm45ubLYqZi8"}, {"cell_type": "markdown", "metadata": {"id": "ohn9nYadqZi9"}, "source": ["Test the inner relationship among political labels"], "id": "ohn9nYadqZi9"}, {"cell_type": "code", "execution_count": null, "metadata": {"vscode": {"languageId": "r"}, "id": "LKOYblXfqZi-", "outputId": "eecc1cd9-dd0c-44a6-9232-fe80062d7630"}, "outputs": [], "source": ["library(lme4)\n", "library(lmerTest) # for ANOVA with mixed models\n", "library(emmeans) # for post hoc tests\n", "emm_options(lmerTest.limit = 61169)\n", "\n", "columns_to_plot <- c('care.virtue', 'fairness.virtue', 'loyalty.virtue',\n", "                     'authority.virtue', 'sanctity.virtue',\n", "                     'care.vice', 'fairness.vice', 'loyalty.vice',\n", "                     'authority.vice', 'sanctity.vice')\n", "\n", "political_labels <- c(\"left\", \"left_center\", \"center\", \"right_center\", \"right\")\n", "\n", "for (column in columns_to_plot) {\n", "  cat(\"Analyzing:\", column, \"\\n\")\n", "\n", "  filtered_data <- merged_df[merged_df$label %in% political_labels, ]\n", "\n", "  formula <- as.formula(paste(column, \"~ label + (1 | Outlet)\"))\n", "  model <- lmer(formula, data = filtered_data)\n", "\n", "  anova_result <- anova(model)\n", "  print(anova_result)\n", "\n", "  if (anova_result$'Pr(>F)'[1] < 0.05) {\n", "    # Perform post hoc tests if the overall test is significant\n", "    post_hoc <- emmeans(model, pairwise ~ label)\n", "    print(post_hoc)\n", "  }\n", "\n", "  cat(\"\\n\") # Newline for readability\n", "}"], "id": "LKOYblXfqZi-"}, {"cell_type": "code", "execution_count": null, "metadata": {"vscode": {"languageId": "r"}, "id": "Byvm9bz6qZi_", "outputId": "5fb658f7-b08e-49c4-814e-77de1e12054b"}, "outputs": [], "source": ["emm_options(pbkrtest.limit = 4461)\n", "\n", "years <- c(2019, 2020, 2021, 2022)\n", "\n", "for (year in years) {\n", "  cat(\"Year:\", year, \"\\n\")\n", "\n", "  year_data <- merged_df[merged_df$Year == year & merged_df$label %in% political_labels, ]\n", "\n", "  for (column in columns_to_plot) {\n", "    cat(\"Analyzing:\", column, \"\\n\")\n", "\n", "    # Fit the mixed-effects model\n", "    formula <- as.formula(paste(column, \"~ label + (1 | Outlet)\"))\n", "    model <- lmer(formula, data = year_data)\n", "\n", "    anova_result <- anova(model)\n", "    print(anova_result)\n", "\n", "    if (anova_result$'Pr(>F)'[1] < 0.05) {\n", "\n", "      post_hoc <- emmeans(model, pairwise ~ label)\n", "      print(post_hoc)\n", "    }\n", "\n", "    cat(\"\\n\")\n", "  }\n", "  cat(\"\\n\")\n", "}"], "id": "Byvm9bz6qZi_"}, {"cell_type": "markdown", "metadata": {"id": "92McUejXqZjA"}, "source": ["2020, 2021 - no significant difference from ANOVA test\n", "2019, 2022 - frequently significant difference from ANOVA test, involving 'center' and other labels"], "id": "92McUejXqZjA"}, {"cell_type": "code", "execution_count": null, "metadata": {"vscode": {"languageId": "r"}, "id": "2zllWsITqZjB", "outputId": "5b9efaae-9e03-49a4-f2b5-4585d12604d0"}, "outputs": [], "source": ["emm_options(pbkrtest.limit = 43137)\n", "columns_to_plot <- c('care.virtue', 'fairness.virtue', 'loyalty.virtue',\n", "                     'authority.virtue', 'sanctity.virtue',\n", "                     'care.vice', 'fairness.vice', 'loyalty.vice',\n", "                     'authority.vice', 'sanctity.vice')\n", "\n", "labels_to_examine <- c(\"left\", \"left_center\", \"center\", \"right_center\", \"right\",\n", "                       \"conspiracy_pseudoscience\", \"questionable_source\",\n", "                       \"satire\", \"pro-science\")\n", "\n", "for (column in columns_to_plot) {\n", "  cat(\"Analyzing:\", column, \"\\n\")\n", "\n", "  filtered_data <- merged_df[merged_df$label %in% labels_to_examine, ]\n", "\n", "  formula <- as.formula(paste(column, \"~ label + (1 | Outlet)\"))\n", "  model <- lmer(formula, data = filtered_data)\n", "\n", "  anova_result <- anova(model)\n", "  print(anova_result)\n", "\n", "  if (anova_result$'Pr(>F)'[1] < 0.05) {\n", "\n", "    post_hoc <- emmeans(model, pairwise ~ label)\n", "    print(post_hoc)\n", "  }\n", "\n", "  cat(\"\\n\")\n", "}"], "id": "2zllWsITqZjB"}, {"cell_type": "markdown", "metadata": {"id": "F_O08aKOqZjC"}, "source": ["conspiracy_pseudoscience is generally higher in all the virtue scores."], "id": "F_O08aKOqZjC"}, {"cell_type": "code", "execution_count": null, "metadata": {"vscode": {"languageId": "r"}, "id": "rOVXt1nQqZjC", "outputId": "63c9e371-4036-4829-e020-bbc8e1e4501e"}, "outputs": [], "source": ["merged_df$care_diff = merged_df$care.virtue - merged_df$care.vice\n", "merged_df$loyalty_diff = merged_df$loyalty.virtue - merged_df$loyalty.vice\n", "merged_df$fairness_diff = merged_df$fairness.virtue - merged_df$fairness.vice\n", "merged_df$authority_diff = merged_df$authority.virtue - merged_df$authority.vice\n", "merged_df$sanctity_diff = merged_df$sanctity.virtue - merged_df$sanctity.vice\n", "\n", "labels_to_examine <- c(\"left\", \"left_center\", \"center\", \"right_center\", \"right\",\n", "                       \"conspiracy_pseudoscience\", \"questionable_source\",\n", "                       \"satire\", \"pro-science\")\n", "diff_scores <- c('care_diff', 'loyalty_diff', 'fairness_diff', 'authority_diff', 'sanctity_diff')\n", "\n", "for (diff_score in diff_scores) {\n", "  cat(\"Analyzing:\", diff_score, \"\\n\")\n", "\n", "  filtered_data <- merged_df[merged_df$label %in% labels_to_examine, ]\n", "\n", "  model <- lmer(as.formula(paste(diff_score, \"~ label + (1 | Outlet)\")), data = filtered_data)\n", "\n", "  anova_result <- anova(model)\n", "  print(anova_result)\n", "\n", "  if (anova_result$'Pr(>F)'[1] < 0.05) {\n", "\n", "    post_hoc <- emmeans(model, pairwise ~ label)\n", "    print(post_hoc)\n", "  }\n", "\n", "  cat(\"\\n\") # Newline for readability\n", "}"], "id": "rOVXt1nQqZjC"}, {"cell_type": "code", "execution_count": null, "id": "4386e7f3", "metadata": {"vscode": {"languageId": "r"}, "id": "4386e7f3", "outputId": "3e309b00-4b7f-4186-fc9b-27ed9acf4d8e"}, "outputs": [], "source": ["library(lme4)\n", "library(lmerTest) # For p-values in the model summary\n", "library(ggplot2)\n", "# install.packages(\"lmtest\")\n", "library(lmtest)\n", "\n", "years <- c(2017, 2018, 2019, 2020, 2021, 2022)\n", "\n", "# Initialize an empty list to store dataframes\n", "all_data <- list()\n", "\n", "# Loop through each year\n", "for (year in years) {\n", "  # Read the CSV file for the current year\n", "  df <- read.csv(paste0(year, \"_doc_mf_scores.csv\"))\n", "\n", "  # Add a 'Year' column\n", "  df$Year <- year\n", "\n", "  # Append the dataframe to the list\n", "  all_data[[length(all_data) + 1]] <- df\n", "}\n", "\n", "merged_df <- do.call(rbind, all_data)\n", "\n", "# If you need to reset row names to avoid duplicate row names\n", "rownames(merged_df) <- NULL\n", "\n", "merged_df$Outlet <- sapply(strsplit(merged_df$Filename, '--'), '[', 1)\n", "\n", "# Columns to analyze\n", "columns_to_plot <- c('care.virtue', 'fairness.virtue', 'loyalty.virtue',\n", "                     'authority.virtue', 'sanctity.virtue',\n", "                     'care.vice', 'fairness.vice', 'loyalty.vice',\n", "                     'authority.vice', 'sanctity.vice')\n", "\n", "# # Convert Doc_Label to a numeric variable named Label_Numeric\n", "# label_numeric <- c('left' = 1, 'left_center' = 2, 'center' = 3, 'right-center' = 4, 'right' = 5)\n", "# # Convert labels of interest into numeric\n", "# merged_df$Label_Numeric <- as.numeric(factor(merged_df$Doc_Label, levels = names(label_numeric)))\n", "\n", "# # Filter to only keep rows with labels of interest\n", "# df_filtered <- merged_df[!is.na(merged_df$Label_Numeric), ]\n", "\n", "\n", "# # Analyze each column\n", "# for (column in columns_to_plot) {\n", "#   formula <- as.formula(paste(column, \"~ Label_Numeric + (1|Outlet)\"))\n", "#   model <- lmer(formula, data = df_filtered)\n", "\n", "#   # Print model summary\n", "#   print(column)\n", "#   print(summary(model))\n", "\n", "# }\n", "\n", "for (year in years) {\n", "  cat(\"Year:\", year, \"\\n\")\n", "\n", "  # Filter the dataframe for the current year\n", "  year_df <- merged_df[merged_df$Year == year, ]\n", "\n", "  # Loop through each column to plot\n", "  for (column in columns_to_plot) {\n", "    # Update the formula with the current column\n", "    # Assuming 'label' is your fixed effect and 'Outlet' (extracted from 'Filename') is your random effect\n", "    formula <- as.formula(paste(column, \"~ label + (1 | Outlet)\"))\n", "\n", "    # Fit the mixed effects model\n", "    model <- lmer(formula, data = year_df)\n", "\n", "    # Print the summary of the model\n", "    print(summary(model))\n", "  }\n", "  cat(\"\\n\")  # Print a newline for better readability between years\n", "}\n", "\n", "full_model <- lmer(as.formula(paste(columns_to_plot[1], \"~ label + (1 | Outlet)\")), data = merged_df)\n", "\n", "# Reduced model without the random effect\n", "reduced_model <- lm(as.formula(paste(columns_to_plot[1], \"~ label\")), data = merged_df)\n", "\n", "# Conduct the likelihood ratio test using lrtest\n", "lr_test <- lrtest(reduced_model, full_model)\n", "\n", "# Print the results of the likelihood ratio test\n", "print(lr_test)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"vscode": {"languageId": "r"}, "id": "bom6LND_qZjE", "outputId": "3d10f405-a248-4488-abdf-a2db1296fe81"}, "outputs": [], "source": ["merged_df"], "id": "bom6LND_qZjE"}, {"cell_type": "code", "execution_count": null, "id": "c5fd62b8", "metadata": {"vscode": {"languageId": "r"}, "id": "c5fd62b8", "outputId": "4226b62f-b5b1-4c97-c89b-a57922dbac1a"}, "outputs": [], "source": ["for (column in columns_to_fit) {\n", "    print(summary(models_list[[column]]))\n", "}"]}, {"cell_type": "code", "execution_count": null, "id": "ff42fe65", "metadata": {"vscode": {"languageId": "r"}, "id": "ff42fe65"}, "outputs": [], "source": ["data <- read.csv(\"updated_doc_mf_scores.csv\")\n", "data$outlet <- str_extract(data$Filename, \"^[^\\\\-\\\\-]*\")\n", "data$Doc_Label <- as.factor(data$Doc_Label)\n", "data$Doc_Label <- relevel(data$Doc_Label, ref = \"center\")\n"]}, {"cell_type": "code", "execution_count": null, "id": "c0dd3778", "metadata": {"vscode": {"languageId": "r"}, "id": "c0dd3778", "outputId": "d70ef07e-18a4-4bf6-8b3d-08d958a6cbf4"}, "outputs": [], "source": ["columns_to_analyze <- c('care.virtue', 'fairness.virtue', 'loyalty.virtue',\n", "                        'authority.virtue', 'sanctity.virtue', 'care.vice',\n", "                        'fairness.vice', 'loyalty.vice', 'authority.vice',\n", "                        'sanctity.vice')\n", "\n", "for(column in columns_to_analyze) {\n", "  formula_string <- paste(column, \"~ Doc_Label + (1|outlet)\")\n", "  formula_obj <- as.formula(formula_string)\n", "\n", "  model <- lmer(formula_obj, data = data)\n", "\n", "  cat(\"\\n\\nSummary for\", column, \":\\n\")\n", "  print(summary(model))\n", "}"]}, {"cell_type": "markdown", "id": "30f16564", "metadata": {"id": "30f16564"}, "source": ["For each of the virtues/vices, there is a non-zero variance for the outlet (Intercept), but the variance is very small and very similar to the Residual Variance. It shows that there is a slight consistency among the articles in the same outlet, but it is likely due to the labeling effect - the variance might due to the difference in the article levels but not in the outlet levels.\n", "\n", "In the labeling level, the t value is small in all cases, so there is no significant effect of different label comparing the center label in all scorings."]}, {"cell_type": "code", "execution_count": null, "id": "9c3597b6", "metadata": {"vscode": {"languageId": "r"}, "id": "9c3597b6"}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "R", "language": "R", "name": "ir"}, "language_info": {"codemirror_mode": "r", "file_extension": ".r", "mimetype": "text/x-r-source", "name": "R", "pygments_lexer": "r", "version": "4.2.2"}, "colab": {"provenance": [{"file_id": "1vqb33Jn7JTU0tUxDsXDS42YMBUOtbcCv", "timestamp": 1716773318513}]}}, "nbformat": 4, "nbformat_minor": 5}