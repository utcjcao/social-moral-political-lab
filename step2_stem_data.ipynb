{"cells": [{"cell_type": "markdown", "metadata": {"id": "m22ruLTX6tOg"}, "source": ["**Instructions**\n", "\n", "The best bet to clean the entire dataset is to use the digital research alliance supercluster to cut down on runtime. The shell files are also included in the google drive\n", "\n", "To run this file, run each cell sequentially from top to bottom. There are cells at the bottom of the notebook which are no longer used, so don't run them. I'm just keeping them there for record.\n", "\n", "**Runtime**\n", "\n", "Cleaning one year has an approximate runtime of ~9 hrs, but it may be faster.\n", "\n", "**Tips**\n", "\n", "Google Colab is prone to disconnecting if your computer falls asleep or your wifi disconnects, so make sure your computer is on and your internet connection is stable."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "N2u1hJxKYUAU", "outputId": "ebad0040-8b97-464f-eb9a-677a1a5a8b8f"}, "outputs": [], "source": ["from google.colab import drive\n", "drive.mount('/content/drive')"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "tiiLAH60FWTG"}, "outputs": [], "source": ["from tqdm import tqdm\n", "import string\n", "import pandas as pd\n", "import nltk"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "B_oOAFt4Id3n"}, "outputs": [], "source": ["project_folder = \"/content/drive/MyDrive/2024SUDSProject/\"\n", "dataset_folder = \"/content/drive/MyDrive/2024SUDSProject/datasets/\""]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "rgFPq2eS0pRq"}, "outputs": [], "source": ["# Creates table of characters removed from text such as numbers and punctuation\n", "\n", "char_removal_dict = {}\n", "\n", "for char in string.printable:\n", "  if char not in string.ascii_letters and char not in string.whitespace:\n", "    char_removal_dict[char] = ''\n", "\n", "char_removal_dict['\\n'] = ''\n", "\n", "removal_table = str.maketrans(char_removal_dict)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "4DFK8jna-Ppz"}, "outputs": [], "source": ["ps = nltk.stem.PorterStemmer()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "NrUKkbjaEu-S", "outputId": "bbb66c74-1ec1-4cac-efad-0e23599cd9f0"}, "outputs": [], "source": ["# Create set of english words for word cleaning\n", "\n", "nltk.download('words')\n", "nltk.download('stopwords')\n", "\n", "from nltk.corpus import words, stopwords\n", "\n", "english_words_set = set(words.words())\n", "stop_words = set(stopwords.words('english'))\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "BkvzMgr3eAee"}, "outputs": [], "source": ["def remove_extra_spaces(text):\n", "    return ' '.join(text.split())\n", "\n", "# Function to remove stopwords and lemmatize from text batch\n", "def remove_stopwords_and_stem_batch(texts):\n", "\n", "    word_tokens = [word_tokenize(text) for text in texts]\n", "\n", "    filtered_docs = []\n", "\n", "    for doc in word_tokens:\n", "        filtered_doc = [ps.stem(word) for word in doc if not word in stop_words\n", "            and word in english_words_set]\n", "        filtered_docs.append(' '.join(filtered_doc))\n", "\n", "    return filtered_docs\n", "\n", "# Cleans a dataframe as a batch. Notice that this method mutates the given dataframe\n", "def clean_dataframe_batch_v2(dataframe):\n", "    dataframe['content'] = remove_stopwords_and_stem_batch(dataframe['content'])"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "PJ2tYwALIYpj", "outputId": "e6d3a020-957d-483e-9a33-fd0569f6d2bb"}, "outputs": [], "source": ["# Define the chunk size (number of rows to read at a time)\n", "chunk_size = 512\n", "\n", "# Counter for debugging purposes\n", "counter = 0\n", "\n", "# Initialize an empty list to store processed chunks\n", "processed_chunks = []\n", "\n", "# years = ['2017', '2018', '2019', '2020', '2021', '2022']\n", "\n", "years = ['2022']\n", "\n", "for year in years:\n", "    # Iterate over chunks of the CSV file\n", "    for chunk in tqdm(pd.read_csv(dataset_folder+f'combined_data_{year}.csv', chunksize=chunk_size), miniters=1, desc='Loading data'):\n", "\n", "        if counter == 4:\n", "          break\n", "\n", "        # Convert all letters to lowercase and remove unnecessary characters\n", "        chunk['content'] = chunk['content'].astype(str).str.lower()\n", "        chunk['content'] = chunk['content'].str.translate(removal_table)\n", "        chunk['content'] = chunk['content'].apply(remove_extra_spaces)\n", "\n", "        # Cleans the dataframe\n", "        clean_dataframe_batch_v2(chunk)\n", "\n", "        # Append the processed chunk to the list\n", "        processed_chunks.append(chunk)\n", "\n", "        counter += 1\n", "\n", "    # Concatenate processed chunks into a single DataFrame\n", "    processed_data = pd.concat(processed_chunks)\n", "\n", "    # Print the first few rows of the processed data\n", "    print(processed_data.head())\n", "\n", "    # Convert dataframe back to csv file\n", "    processed_data.to_csv(dataset_folder+f\"combined_data_preprocessed_{year}_stem.csv\", index=False)\n"]}], "metadata": {"colab": {"provenance": [{"file_id": "1a5x4EMBWMWIkbYHGPyViF97Dcqzgyr-C", "timestamp": 1716768345708}]}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.7"}}, "nbformat": 4, "nbformat_minor": 0}