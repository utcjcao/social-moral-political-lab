{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "Zz-aRK4wU_d3", "outputId": "dd22416f-7421-4e5a-f4a9-5f32dead9b3a"}, "outputs": [], "source": ["# Connect with drive\n", "from google.colab import drive\n", "drive.mount('/content/drive')"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "SsmO0uXQXoc1"}, "outputs": [], "source": ["# Import libraries\n", "import pandas as pd\n", "from gensim import corpora, models, utils\n", "import numpy as np\n", "import pickle\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "from gensim.models import KeyedVectors\n", "import os"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "ZYKLrEBCVM3r"}, "outputs": [], "source": ["# Address to project folder\n", "project_folder = \"/content/drive/MyDrive/2024SUDSProject/Morality/\"\n", "dataset_folder = \"/content/drive/MyDrive/2024SUDSProject/processedNewsData/\""]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "Ki8qiE0WtT7w"}, "outputs": [], "source": ["# Declaring labels\n", "scope = [\n", "    'care.virtue', 'fairness.virtue', 'loyalty.virtue', 'authority.virtue', 'sanctity.virtue',\n", "    'care.vice', 'fairness.vice', 'loyalty.vice', 'authority.vice', 'sanctity.vice'\n", "]\n", "\n", "label_order = [\n", "    'left', 'left-center', 'center', 'right-center', 'right', 'extreme-right', 'conspiracy-pseudoscience','questionable-source'\n", "]\n", "\n", "# Assign a year\n", "years = ['2017', '2018', '2019', '2020', '2021', '2022']\n", "\n", "years = ['2019', '2020', '2021', '2022']"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 321}, "id": "P08JJ2qu_kpU", "outputId": "e8926468-c873-47c5-84b6-abb77d0b048f"}, "outputs": [], "source": ["# Open moral foundation dictionary, set the index to the word\n", "emfd = pd.read_csv(dataset_folder + 'emfd_amp.csv')\n", "\n", "labels = pd.read_csv(dataset_folder + 'labels_all_2022.csv')\n", "\n", "#Calculate the score for each word\n", "word2vec_model = KeyedVectors.load_word2vec_format(dataset_folder+'GoogleNews-vectors-negative300.bin.gz', binary=True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "lko34csgBeHp"}, "outputs": [], "source": ["def create_emfd_priori():\n", "    foundation_to_index = {foundation: idx for idx, foundation in enumerate(scope)}\n", "\n", "    # Compiles dictionary to find the top 10 words associated with\n", "    # each moral foundation category, stores the within moral_foundation_seed_dict\n", "\n", "    emfd_priori = {}\n", "\n", "    for foundation in scope:\n", "        # Find columns related to the current foundation\n", "        # Collect top 10 words from each relevant column\n", "        # 27 is the optimal number for least amount of overlap with most amount of unique words per category\n", "        top_words = emfd.nlargest(27, foundation)['word']\n", "\n", "        # Map each top word to the current foundation index\n", "        for word in top_words:\n", "            if word not in emfd_priori:\n", "                emfd_priori[word] = []\n", "            emfd_priori[word].append(foundation_to_index[foundation])\n", "\n", "    return emfd_priori\n", "\n", "def open_emfd_priori():\n", "    if os.path.exists(dataset_folder + f'step4_emfd_priori.pkl'):\n", "        with open(dataset_folder + f'step4_emfd_priori.pkl', 'rb') as file:\n", "            emfd_priori = pickle.load(file)\n", "        return emfd_priori\n", "    else:\n", "        emfd_priori = create_emfd_priori()\n", "        with open(dataset_folder + f'step4_emfd_priori.pkl', 'wb') as file:\n", "            pickle.dump(emfd_priori, file, protocol=pickle.HIGHEST_PROTOCOL)\n", "        return emfd_priori"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "uQfW2_3-XqUr"}, "outputs": [], "source": ["def open_bow(year):\n", "    with open(dataset_folder + f'step3_bow_{year}.pkl', 'rb') as file:\n", "        bow = pickle.load(file)\n", "    return bow\n", "\n", "def open_dict(year):\n", "    with open(dataset_folder + f'step3_dictionary_{year}.pkl', 'rb') as file:\n", "        dictionary = pickle.load(file)\n", "    return dictionary"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "I70ujD9-YgOq"}, "outputs": [], "source": ["def initialize_guidedlda_v2(eta, dictionary, bow, ntopics, year):\n", "    file_path = dataset_folder + f'{year}_lda_model_gensim.pkl'\n", "    if os.path.exists(file_path):\n", "        with open(file_path, 'rb') as file:\n", "            print('model loaded')\n", "            model = pickle.load(file)\n", "\n", "        return model\n", "\n", "    np.random.seed(42) # set the random seed for repeatability\n", "    print('model created')\n", "    with (np.errstate(divide='ignore')):  # ignore divide-by-zero warnings\n", "        model = models.ldamulticore.LdaMulticore(\n", "            corpus=bow, id2word=dictionary, num_topics=ntopics,\n", "            random_state=42, eta=eta,\n", "            eval_every=-1,\n", "            passes=150, per_word_topics=False)\n", "\n", "    return model\n", "\n", "def create_eta(priors, etadict, ntopics):\n", "    eta = np.full(shape=(ntopics, len(etadict)), fill_value=1) # create a (ntopics, nterms) matrix and fill with 1\n", "    for word, topics in priors.items(): # for each word in the list of priors\n", "        keyindex = [index for index,term in etadict.items() if term==word] # look up the word in the dictionary\n", "        if (len(keyindex)>0): # if it's in the dictionary\n", "            for topic in topics:\n", "              eta[topic,keyindex[0]] = 1e7  # put a large number in there\n", "    eta = np.divide(eta, eta.sum(axis=0)) # normalize so that the probabilities sum to 1 over all topics\n", "\n", "    return eta"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "a5Z394VapTLv"}, "outputs": [], "source": ["def get_topic_words_and_contributions(model, dictionary):\n", "\n", "    topic_word_distributions = model.get_topics()\n", "\n", "    # We specify the number of top words we want from each topic\n", "    num_top_words = 100\n", "\n", "    # Get total vocabulary of dictionary, so all the words that we've seen so far\n", "    feature_names = list(dictionary.token2id.keys())\n", "\n", "    return [get_top_word_distributions(topic_word_distribution, feature_names, num_top_words) for topic_word_distribution in topic_word_distributions]\n", "\n", "def get_top_word_distributions(topic_word_distribution, feature_names, n_top_words):\n", "    # Sorts indices of the topic-word distribution array in descending order of value\n", "    # Selects indices corresponding to the top 'n_top_words' words for each topic\n", "    top_word_indices = topic_word_distribution.argsort()[:-n_top_words - 1:-1]\n", "\n", "    # Retrieves the corresponding words from the feature names array using the selected indices\n", "    top_words = [feature_names[i] for i in top_word_indices]\n", "\n", "    # Computes contributions of each top word to its respective topic\n", "    # It divides the probabilities of the top words by the sum of probabilities for all words in each topic.\n", "    contributions = topic_word_distribution[top_word_indices] / topic_word_distribution.sum()\n", "    # TODO: try swapping this for the total probability of top 100 words instead\n", "\n", "    # Pairs each top word with its corresponding contribution to the topic\n", "    return list(zip(top_words, contributions))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "-IBCNZNNqiVW"}, "outputs": [], "source": ["# Calculates the similarity score of a word to the moral foundation dictionary\n", "def get_emfd_scores(word):\n", "    word_scores = {moral_foundation_category: 0 for moral_foundation_category in scope}\n", "\n", "    # Check if the word is in the moral foundation dictionary\n", "    if word in emfd.index:\n", "        word_scores = in_emfd_score(word_scores, word)\n", "    else:\n", "        word_scores = out_emfd_score(word_scores, word)\n", "\n", "    return word_scores\n", "\n", "def in_emfd_score(word_scores, word):\n", "    for moral_foundation_category in scope:\n", "            # Adds the corresponding value for that word in emfd to word_scores\n", "            word_scores[moral_foundation_category] += emfd.loc[word, moral_foundation_category]\n", "\n", "    return word_scores\n", "\n", "def out_emfd_score(word_scores, word):\n", "    sims = []\n", "\n", "    for emfd_word in emfd.index:\n", "        try:\n", "            # Calculate similarity between the word and each word in the moral foundation dictionary\n", "            sim = word2vec_model.similarity(word, emfd_word)\n", "            sims.append((emfd_word, sim))\n", "        except:\n", "            # If similarity calculation fails, skip ahead\n", "            continue\n", "\n", "    # Get top 10 most similar words\n", "    top_10_words = sorted(sims, key=lambda x: x[1], reverse=True)[:10]\n", "    total_sim_weight = sum([sim for word, sim in top_10_words])\n", "\n", "    # If total_sim_weight > 0, then there are valid similarity scores.\n", "    if total_sim_weight > 0:\n", "        for word, sim in top_10_words:\n", "            for moral_foundation_category in scope:\n", "                word_scores[moral_foundation_category] += (emfd.loc[word, moral_foundation_category] * sim) / total_sim_weight\n", "\n", "    return word_scores\n", "\n", "# Given the topic_top_words_and_contributions, appends the extended moral\n", "# foundation dictionary score, and returns it as a dataframe\n", "\n", "def find_word_emfd_scores_df(topic_top_words_and_contributions):\n", "    word_data_list = []\n", "\n", "    for topic_index, topic in enumerate(topic_top_words_and_contributions):\n", "        for word, contribution in topic:\n", "            word_contribution_dict = {'Topic': topic_index, 'Word': word, 'Contribution': contribution}\n", "            word_contribution_dict.update(get_emfd_scores(word))\n", "            word_data_list.append(word_contribution_dict)\n", "\n", "    return pd.DataFrame(word_data_list)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "MQRQga7A-GVb"}, "outputs": [], "source": ["# Save LDA model\n", "def save_LDA_model(model, year):\n", "    with open(project_folder+f'models/{year}_lda_model_gensim.pkl', 'wb') as file:\n", "        pickle.dump(model, file, protocol=pickle.HIGHEST_PROTOCOL)\n", "\n", "def save_doc_topic_distribution_df(doc_topic_dist_df, year):\n", "  doc_topic_dist_df.to_csv(dataset_folder + f'doc_topic_dist_{year}.csv', index=False)\n", "\n", "def save_doc_mf_scores_df(doc_mf_scores_df, year):\n", "    doc_mf_scores_df.to_csv(dataset_folder + f'doc_mf_scores_{year}.csv', index=False)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "Fe3fcYLSJPqG"}, "outputs": [], "source": ["# Group the data by topic and calculate the average contribution for each scope.\n", "def get_topic_scores(word_scores_df):\n", "    # Start by initializing an empty list to store results\n", "    results = []\n", "\n", "    # Iterate over unique topics\n", "    for topic in word_scores_df['Topic'].unique():\n", "        group = word_scores_df[word_scores_df['Topic'] == topic]\n", "        topic_result = {}\n", "\n", "        # Calculate weighted averages for each column in scope\n", "        for column in scope:\n", "            weighted_avg = np.average(group[column], weights=group['Contribution'])\n", "            topic_result[column] = weighted_avg\n", "\n", "        # Append the result for the current topic to the results list\n", "        results.append(topic_result)\n", "\n", "    topic_scores = pd.DataFrame(results)\n", "\n", "    # Resets the indexes of topic_scores\n", "    topic_scores.reset_index(inplace=True, drop=True)\n", "\n", "    # Convert results list of dictionaries into a DataFrame\n", "    return topic_scores"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "pn6HFx0yuOM1"}, "outputs": [], "source": ["def get_topic_dist_df(bow):\n", "    doc_topic_dists = [model.get_document_topics(doc, minimum_probability=0) for doc in bow]\n", "\n", "    # Create a DataFrame from the topic distributions\n", "\n", "    topic_matrix = []\n", "    for doc_topics in doc_topic_dists:\n", "        row = [prob for _, prob in doc_topics]\n", "        topic_matrix.append(row)\n", "\n", "    doc_topic_dist_df = pd.DataFrame(topic_matrix)\n", "\n", "    # Add the filename column and format the dataframe\n", "    labels = pd.read_csv(dataset_folder+f'combined_data_preprocessed_{year}_lemma.csv', usecols=['id'])\n", "\n", "    doc_topic_dist_df['Filename'] = labels['id']\n", "    doc_topic_dist_df['Topic_Distribution'] = doc_topic_dist_df.iloc[:, :-3].max(axis=1)\n", "\n", "    return doc_topic_dist_df"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "tFsxr1szwkoh"}, "outputs": [], "source": ["def create_doc_mf_scores(doc_topic_dist_df, topic_scores):\n", "    doc_mf_scores_list = []\n", "\n", "    # Iterate over each document\n", "    for index, row in doc_topic_dist_df.iterrows():\n", "\n", "        # Dictionary to store the moral foundation score for the current document\n", "        doc_mf_scores = {'Filename': row['Filename']}\n", "\n", "        # Getting topic distribution for the current document\n", "        topic_distribution = row[:-1]  # adjust the index to match your data\n", "\n", "        # Calculate the moral foundation score for the current document\n", "        for category in scope:\n", "            weighted_score = sum(\n", "                topic_distribution[i] * topic_scores.loc[i, category]\n", "                for i in range(len(topic_scores))\n", "            )\n", "            doc_mf_scores[category] = weighted_score\n", "\n", "        # Append the scores dictionary to the doc_mf_scores_list\n", "        doc_mf_scores_list.append(doc_mf_scores)\n", "\n", "    # Create a new dataframe from the doc_mf_scores_list\n", "    doc_mf_scores_df = pd.DataFrame(doc_mf_scores_list)\n", "    # doc_mf_scores_df.to_csv(project_folder + f'data/{year}_doc_mf_scores_1.csv', index=False)\n", "\n", "    doc_mf_scores_df['outlet'] = doc_mf_scores_df['Filename'].str.split('--').str[0]\n", "    doc_mf_scores_df['label'] = doc_mf_scores_df.apply(get_label, axis=1)\n", "\n", "    return doc_mf_scores_df\n", "\n", "def save_doc_mf_scores_df(doc_mf_scores_df, year):\n", "    doc_mf_scores_df.to_csv(dataset_folder + f'doc_mf_scores_{year}.csv', index=False)\n", "\n", "def get_label(x):\n", "    values = labels.loc[labels['source'] == x['outlet'], 'bias'].values\n", "    return values[0] if len(values) > 0 else None"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "etTj0b7UDztR"}, "outputs": [], "source": ["def open_dict(year):\n", "    with open(dataset_folder + f'step3_dictionary_{year}.pkl', 'rb') as file:\n", "        dictionary = pickle.load(file)\n", "    return dictionary"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 248}, "id": "Qz7gK_Tw68aS", "outputId": "8fde796b-3381-486b-bd03-e64048fc1065"}, "outputs": [], "source": ["years = ['2019']\n", "\n", "emfd_priori = open_emfd_priori()\n", "print(emfd_priori)\n", "# emfd.set_index('word', inplace=True)\n", "\n", "for year in years:\n", "    dictionary = open_dict(year)\n", "    eta = create_eta(emfd_priori, dictionary, ntopics=10)\n", "    bow = open_bow(year)\n", "\n", "    # Initialize and train the model\n", "\n", "    model = initialize_guidedlda_v2(eta, dictionary, bow, 10, year)\n", "\n", "    # save_LDA_model(model, year)\n", "\n", "    topic_words_and_contributions = get_topic_words_and_contributions(model, dictionary)\n", "\n", "    word_scores_df = find_word_emfd_scores_df(topic_words_and_contributions)\n", "\n", "    # Get topic scores\n", "    topic_scores = get_topic_scores(word_scores_df)\n", "\n", "    doc_topic_dist_df = get_topic_dist_df(bow)\n", "    # save_doc_topic_distribution_df(doc_topic_dist_df, year)\n", "\n", "    # Create a dataframe of the moral foundation scores for each document\n", "    doc_mf_scores_df = create_doc_mf_scores(doc_topic_dist_df, topic_scores)\n", "    # save_doc_mf_scores_df(doc_mf_scores_df, year)"]}], "metadata": {"colab": {"machine_shape": "hm", "provenance": [{"file_id": "1WW39JkF807d7KpE_al_GUe9rgvkko5QB", "timestamp": 1716772355788}]}, "kernelspec": {"display_name": "Python 3", "name": "python3"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 0}